{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KSpicer\\Anaconda3\\envs\\python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: fitzgerald_all_the_sad.txt\n",
      "Processing file: fitzgerald_bablyon_revisited.txt\n",
      "Processing file: fitzgerald_beautiful_and_damned.txt\n",
      "Processing file: fitzgerald_flappers_and_philosophers.txt\n",
      "Processing file: fitzgerald_gatsby.txt\n",
      "Processing file: fitzgerald_tales_jazz_age.txt\n",
      "Processing file: fitzgerald_tender_is.txt\n",
      "Processing file: fitzgerald_the_vegtable.txt\n",
      "Processing file: fitzgerald_this_side.txt\n",
      "Processing file: hemingway_across_the_river.txt\n",
      "Processing file: hemingway_bell_tolls.txt\n",
      "Processing file: hemingway_farewell.txt\n",
      "Processing file: hemingway_green_hills_africa.txt\n",
      "Processing file: hemingway_in_our_time.txt\n",
      "Processing file: hemingway_men_without_women.txt\n",
      "Processing file: hemingway_old_man.txt\n",
      "Processing file: hemingway_sun_also.txt\n",
      "Processing file: hemingway_three_stories_ten_poems.txt\n",
      "Processing file: hemingway_winner_take_nothing.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from helper_functions import load_data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper, text.split()))\n",
    "\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('\\s+', ' ', sent) # remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent\n",
    "\n",
    "def count_direct_objects(text):\n",
    "    print(\"Processing ...\")\n",
    "    doc_text = nlp(text)\n",
    "    direct_object_counts = 0\n",
    "    print(\"POS tagging and counting direct objects ...\")\n",
    "    for chunk in doc_text.noun_chunks:\n",
    "        if chunk.root.dep_ == 'dobj':\n",
    "            direct_object_counts += 1\n",
    "    return(direct_object_counts)\n",
    "\n",
    "# Load the data\n",
    "text_data, labels = load_data('data')\n",
    "df = pd.DataFrame(list(zip(text_data, labels)), columns=['text_data', 'label'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "'''\n",
    "for idx, row in train_df.iterrows():\n",
    "    train_df.loc[idx, 'chars'] = count_chars(row['text_data'])\n",
    "    train_df.loc[idx, 'words'] = count_words(row['text_data'])\n",
    "    train_df.loc[idx, 'capital_words'] = count_capital_words(row['text_data'])\n",
    "    train_df.loc[idx, 'sentence_count'] = count_sent(row['text_data'])\n",
    "    train_df.loc[idx, 'unique_words'] = count_unique_words(row['text_data'])\n",
    "    train_df.loc[idx, 'stopwords_count'] = count_stopwords(row['text_data'])\n",
    "    train_df.loc[idx, 'direct_objects_count'] = count_direct_objects(row['text_data'])\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    test_df.loc[idx, 'chars'] = count_chars(row['text_data'])\n",
    "    test_df.loc[idx, 'words'] = count_words(row['text_data'])\n",
    "    test_df.loc[idx, 'capital_words'] = count_capital_words(row['text_data'])\n",
    "    test_df.loc[idx, 'sentence_count'] = count_sent(row['text_data'])\n",
    "    test_df.loc[idx, 'unique_words'] = count_unique_words(row['text_data'])\n",
    "    test_df.loc[idx, 'stopwords_count'] = count_stopwords(row['text_data'])\n",
    "    test_df.loc[idx, 'direct_objects_count'] = count_direct_objects(row['text_data']) \n",
    "'''\n",
    "\n",
    "# add engineered features with numerical indices\n",
    "train_df['feat_0'] = train_df['text_data'].apply(count_chars)\n",
    "train_df['feat_1'] = train_df['text_data'].apply(count_words)\n",
    "train_df['feat_2'] = train_df['text_data'].apply(count_capital_words)\n",
    "train_df['feat_3'] = train_df['text_data'].apply(count_sent)\n",
    "train_df['feat_4'] = train_df['text_data'].apply(count_unique_words)\n",
    "train_df['feat_5'] = train_df['text_data'].apply(count_stopwords)\n",
    "train_df['feat_6'] = train_df['text_data'].apply(count_direct_objects)\n",
    "\n",
    "test_df['feat_0'] = test_df['text_data'].apply(count_chars)\n",
    "test_df['feat_1'] = test_df['text_data'].apply(count_words)\n",
    "test_df['feat_2'] = test_df['text_data'].apply(count_capital_words)\n",
    "test_df['feat_3'] = test_df['text_data'].apply(count_sent)\n",
    "test_df['feat_4'] = test_df['text_data'].apply(count_unique_words)\n",
    "test_df['feat_5'] = test_df['text_data'].apply(count_stopwords)\n",
    "test_df['feat_6'] = test_df['text_data'].apply(count_direct_objects)\n",
    "\n",
    "# Create a TfidfVectorizer object and fit it on the training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_tf_idf = vectorizer.fit_transform(train_df['text_data']).toarray()\n",
    "\n",
    "# Apply the same vectorizer to the test data\n",
    "test_tf_idf = vectorizer.transform(test_df['text_data']).toarray()\n",
    "# Combine the tf-idf features with other engineered features\n",
    "engineered_features = ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6']\n",
    "train_X = np.concatenate([train_tf_idf, train_df[engineered_features].values], axis=1)\n",
    "test_X = np.concatenate([test_tf_idf, test_df[engineered_features].values], axis=1)\n",
    "\n",
    "'''\n",
    "# Combine the tf-idf features with other engineered features\n",
    "train_X = np.concatenate([train_tf_idf, train_df[['chars', 'words', 'capital_words', 'sentence_count', 'unique_words', 'stopwords_count', 'direct_objects_count']].values], axis=1)\n",
    "test_X = np.concatenate([test_tf_idf, test_df[['chars', 'words', 'capital_words', 'sentence_count', 'unique_words', 'stopwords_count', 'direct_objects_count']].values], axis=1)\n",
    "'''\n",
    "\n",
    "# Extract the labels for the training and test sets\n",
    "train_Y = train_df['label']\n",
    "test_Y = test_df['label']\n",
    "\n",
    "# Train a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_X, train_Y)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "pred_Y = rf.predict(test_X)\n",
    "\n",
    "# Print the classification report and confusion matrix\n",
    "print(classification_report(test_Y, pred_Y))\n",
    "print(confusion_matrix(test_Y, pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assume cm is the confusion matrix\n",
    "cm = confusion_matrix(test_Y, pred_Y)\n",
    "\n",
    "# create a list of class labels\n",
    "classes = ['fitzgerald', 'hemingway']\n",
    "\n",
    "# plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "# add axis labels and title\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "\n",
    "# Sort the features by their importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(train_tf_idf.shape[1]):\n",
    "    print(f\"{f+1}. feature {indices[f]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_features(feature_importances, feature_names, n):\n",
    "    # Create a list of tuples with feature names and importance scores\n",
    "    features = list(zip(feature_names, feature_importances))\n",
    "\n",
    "    # Sort the list by importance score in descending order\n",
    "    features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top n features\n",
    "    return features[:n]\n",
    "\n",
    "top_features = get_top_n_features(rf.feature_importances_, vectorizer.get_feature_names_out(), n=25)\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a mapping between feature indices and feature names\n",
    "feature_mapping = {}\n",
    "for feature_index in range(len(feature_names)):\n",
    "    feature_mapping[feature_index] = feature_names[feature_index]\n",
    "\n",
    "# Print out the feature mapping\n",
    "print(feature_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = rf.feature_importances_\n",
    "feature_importance_dict = {}\n",
    "desired_features = ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6']\n",
    "\n",
    "for feature, importance in zip(desired_features, feature_importances):\n",
    "    feature_importance_dict[feature] = importance\n",
    "\n",
    "# Print the feature importances\n",
    "for feature, importance in feature_importance_dict.items():\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# predict probabilities for test set\n",
    "probs = rf.predict_proba(test_X)[:, 1]\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_Y, probs)\n",
    "\n",
    "# calculate AUC score\n",
    "auc = roc_auc_score(test_Y, probs)\n",
    "\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (AUC = {:.3f})'.format(auc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4159a6653fa75438f66a7bfc0596de9be5c1734cbef8b4e22353e9d3f86b82c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
