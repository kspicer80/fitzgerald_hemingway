{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'data\\fitzgerald\\fitzgerald_gatsby.txt', 'r') as f:\n",
    "    fg_text = f.read()\n",
    "\n",
    "gatsby_nlp = nlp(fg_text)\n",
    "\n",
    "with open(r'data\\hemingway\\hemingway_sun_also.txt', 'r') as f:\n",
    "    sun_text = f.read()\n",
    "\n",
    "sun_nlp = nlp(sun_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(gatsby_nlp.sents)))\n",
    "print(len(list(sun_nlp.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagDict = {w.pos: w.pos_ for w in gatsby_nlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extendedTagDict = {w.pos: w.pos_ + \"_\" + w.tag_ for w in gatsby_nlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatsby_POS = pd.Series(gatsby_nlp.count_by(spacy.attrs.POS))/len(gatsby_nlp)\n",
    "sun_POS = pd.Series(sun_nlp.count_by(spacy.attrs.POS))/len(sun_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([gatsby_POS, sun_POS], index=['fitzgerald', 'hemingway'])\n",
    "df.columns = [tagDict[column] for column in df.columns]\n",
    "df.T.plot(kind='bar')\n",
    "plt.title('All the Different Kinds of Parts of Speech', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([gatsby_POS, sun_POS], index=['fitzgerald', 'hemingway'])\n",
    "df.columns = [extendedTagDict[column] for column in df.columns]\n",
    "df.T.plot(kind='bar')\n",
    "plt.title('All the Different Kinds of Parts of Speech', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    'data/fitzgerald/fitzgerald_gatsby.txt',\n",
    "    'data/hemingway/hemingway_sun_also.txt'\n",
    "]\n",
    "\n",
    "# List to store the number of direct objects for each text file\n",
    "direct_object_counts = []\n",
    "\n",
    "# Loop over the file names\n",
    "for file_name in file_names:\n",
    "    # Open the text file and read its content\n",
    "    with open(file_name, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize a counter for direct objects\n",
    "    direct_object_count = 0\n",
    "\n",
    "    # Iterate over the parsed sentences in the text\n",
    "    for sent in doc.sents:\n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sent:\n",
    "            # Check if the token is a direct object\n",
    "            if token.dep_ == \"dobj\":\n",
    "                # Increment the counter\n",
    "                direct_object_count += 1\n",
    "\n",
    "    # Append the number of direct objects found to the list\n",
    "    direct_object_counts.append(direct_object_count)\n",
    "\n",
    "# Plot the results using a bar plot\n",
    "x = range(len(file_names))\n",
    "plt.bar(x, direct_object_counts)\n",
    "plt.xticks(x, file_names)\n",
    "plt.ylabel(\"Number of Direct Objects\")\n",
    "plt.title(\"Direct Object Counts in Text Files\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_object_counts = []\n",
    "punct_counts = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    # Open the text file and read its content\n",
    "    with open(file_name, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize a counter for direct objects\n",
    "    direct_object_count = 0\n",
    "    punct_count = 0\n",
    "\n",
    "    # Iterate over the parsed sentences in the text\n",
    "    for sent in doc.sents:\n",
    "        # Iterate over the tokens in the sentence\n",
    "        for token in sent:\n",
    "            # Check if the token is a direct object\n",
    "            if token.dep_ == \"dobj\":\n",
    "                # Increment the counter\n",
    "                direct_object_count += 1\n",
    "            elif token.pos_ == \"PUNCT\":\n",
    "                punct_count += 1\n",
    "\n",
    "    # Append the number of direct objects found to the list\n",
    "    direct_object_counts.append(direct_object_count)\n",
    "    punct_counts.append(punct_count)\n",
    "\n",
    "x = range(len(file_names))\n",
    "bar_width = 0.15\n",
    "plt.bar(x, direct_object_counts, bar_width, label=\"Direct Objects\")\n",
    "plt.bar([i + bar_width for i in x], punct_counts, bar_width, label=\"Punctuation Marks\")\n",
    "plt.xticks(x, file_names)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Direct Object and Punctuation Mark Counts in Text Files\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "root_folder = 'data'\n",
    "filenames = []\n",
    "direct_object_counts = []\n",
    "punct_counts = []\n",
    "\n",
    "for subfolder in os.listdir(root_folder):\n",
    "    subfolder_path = os.path.join(root_folder, subfolder)\n",
    "    \n",
    "    for file in os.listdir(subfolder_path):\n",
    "        file_path = os.path.join(subfolder_path, file)\n",
    "        print(\"Processing file:\", file)\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        filename_cut = file_path.split(\"\\\\\")[2].split(\".\")[0]\n",
    "        filenames.append(filename_cut)\n",
    "        \n",
    "        # Process the text using spaCy\n",
    "        print(\"Converting to spaCy doc format:\", file)\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Initialize a counter for direct objects\n",
    "        direct_object_count = 0\n",
    "        punct_count = 0\n",
    "\n",
    "        # Iterate over the parsed sentences in the text\n",
    "        for sent in doc.sents:\n",
    "            # Iterate over the tokens in the sentence\n",
    "            for token in sent:\n",
    "                # Check if the token is a direct object\n",
    "                if token.dep_ == \"dobj\":\n",
    "                    # Increment the counter\n",
    "                    direct_object_count += 1\n",
    "                elif token.pos_ == \"PUNCT\":\n",
    "                    punct_count += 1\n",
    "\n",
    "    # Append the number of direct objects found to the list\n",
    "        direct_object_counts.append(direct_object_count)\n",
    "        punct_counts.append(punct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize each text based on author\n",
    "author_category = []\n",
    "for filename in filenames:\n",
    "    if \"fitzgerald\" in filename.lower():\n",
    "        author_category.append(\"Fitzgerald\")\n",
    "    elif \"hemingway\" in filename.lower():\n",
    "        author_category.append(\"Hemingway\")\n",
    "    else:\n",
    "        author_category.append(\"Other\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "# Plot the direct object counts for each text\n",
    "for i, filename in enumerate(filenames):\n",
    "    if author_category[i] == \"Fitzgerald\":\n",
    "        color = \"red\"\n",
    "    elif author_category[i] == \"Hemingway\":\n",
    "        color = \"blue\"\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "    ax1.bar(filename, direct_object_counts[i], color=color)\n",
    "ax1.set_xticklabels(filenames, rotation=90)\n",
    "ax1.set_title(\"Direct Object Counts by Author\")\n",
    "\n",
    "# Plot the punctuation counts for each text\n",
    "for i, filename in enumerate(filenames):\n",
    "    if author_category[i] == \"Fitzgerald\":\n",
    "        color = \"red\"\n",
    "    elif author_category[i] == \"Hemingway\":\n",
    "        color = \"blue\"\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "    ax2.bar(filename, punct_counts[i], color=color)\n",
    "ax2.set_xticklabels(filenames, rotation=90)\n",
    "ax2.set_title(\"Punctuation Counts by Author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Categorize each text based on author\n",
    "author_category = []\n",
    "for filename in filenames:\n",
    "    if \"fitzgerald\" in filename.lower():\n",
    "        author_category.append(\"Fitzgerald\")\n",
    "    elif \"hemingway\" in filename.lower():\n",
    "        author_category.append(\"Hemingway\")\n",
    "    else:\n",
    "        author_category.append(\"Other\")\n",
    "\n",
    "# Plot the direct object counts for each text\n",
    "fig, ax = plt.subplots()\n",
    "for i, filename in enumerate(filenames):\n",
    "    if author_category[i] == \"Fitzgerald\":\n",
    "        color = \"red\"\n",
    "    elif author_category[i] == \"Hemingway\":\n",
    "        color = \"blue\"\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "    ax.bar(filename, direct_object_counts[i], color=color)\n",
    "ax.set_xticklabels(filenames, rotation=90)\n",
    "ax.set_title(\"Direct Object Counts by Author\")\n",
    "\n",
    "# Plot the punctuation counts for each text\n",
    "fig, ax = plt.subplots()\n",
    "for i, filename in enumerate(filenames):\n",
    "    if author_category[i] == \"Fitzgerald\":\n",
    "        color = \"red\"\n",
    "    elif author_category[i] == \"Hemingway\":\n",
    "        color = \"blue\"\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "    ax.bar(filename, punct_counts[i], color=color)\n",
    "ax.set_xticklabels(filenames, rotation=90)\n",
    "ax.set_title(\"Punctuation Counts by Author\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))\n",
    "\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('\\s+', ' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions import load_data\n",
    "import pandas as pd\n",
    "\n",
    "text_data, labels = load_data('data')\n",
    "\n",
    "df = pd.DataFrame(list(zip(text_data, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "df['text_data'] = df['text_data'].apply(lambda x: preprocess(x))\n",
    "df['text_data'] = df['text_data'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No feature engineering whatsoever ...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_final_features = vectorizer.fit_transform(train['text_data']).toarray()\n",
    "test_final_features = vectorizer.transform(test['text_data']).toarray()\n",
    "print(train_final_features.shape)\n",
    "print(test_final_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf = pd.DataFrame(train_final_features)\n",
    "test_tf_idf = pd.DataFrame(test_final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['label']\n",
    "test_label = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tf_idf, train_label, test_size = 0.2)\n",
    "ytest = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_tf_idf, train_label ,test_size=0.2)\n",
    "_RandomForestClassifier = RandomForestClassifier(n_estimators = 1000, min_samples_split = 15, random_state = 42)\n",
    "_RandomForestClassifier.fit(X_train, y_train)\n",
    "_RandomForestClassifier_prediction = _RandomForestClassifier.predict(X_test)\n",
    "val_RandomForestClassifier_prediction = _RandomForestClassifier.predict(test_tf_idf)\n",
    "print(\"Accuracy => \", round(accuracy_score(_RandomForestClassifier_prediction, y_test)*100, 2))\n",
    "print(\"\\nRandom Forest Classifier results: \\n\")\n",
    "print(classification_report(y_test, _RandomForestClassifier_prediction, target_names = ['fitzgerald', 'hemingway']))\n",
    "print(\"Validation Accuracy => \", round(accuracy_score(val_RandomForestClassifier_prediction, ytest)*100, 2))\n",
    "print(\"\\nValidation Random Forest Classifier results: \\n\")\n",
    "print(classification_report(ytest, val_RandomForestClassifier_prediction, target_names = ['fitzgerald', 'hemingway']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions import load_data\n",
    "import pandas as pd\n",
    "\n",
    "text_data, labels = load_data('data')\n",
    "\n",
    "df = pd.DataFrame(list(zip(text_data, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={0: \"text_data\", 1: \"label\"})\n",
    "test = test.rename(columns={0: \"text_data\", 1: \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in train.iterrows():\n",
    "    train.loc[idx, 'chars'] = count_chars(row['text_data'])\n",
    "    train.loc[idx, 'words'] = count_words(row['text_data'])\n",
    "    train.loc[idx, 'capital_words'] = count_capital_words(row['text_data'])\n",
    "    train.loc[idx, 'sentence_count'] = count_sent(row['text_data'])\n",
    "    train.loc[idx, 'unique_words'] = count_unique_words(row['text_data'])\n",
    "    train.loc[idx, 'stopwords_count'] = count_stopwords(row['text_data'])\n",
    "\n",
    "for idx, row in test.iterrows():\n",
    "    test.loc[idx, 'chars'] = count_chars(row['text_data'])\n",
    "    test.loc[idx, 'words'] = count_words(row['text_data'])\n",
    "    test.loc[idx, 'capital_words'] = count_capital_words(row['text_data'])\n",
    "    test.loc[idx, 'sentence_count'] = count_sent(row['text_data'])\n",
    "    test.loc[idx, 'unique_words'] = count_unique_words(row['text_data'])\n",
    "    test.loc[idx, 'stopwords_count'] = count_stopwords(row['text_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_tf_idf_features = vectorizer.fit_transform(train['text_data']).toarray()\n",
    "test_tf_idf_features = vectorizer.transform(test['text_data']).toarray()\n",
    "print(train_tf_idf_features.shape)\n",
    "print(test_tf_idf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf = pd.DataFrame(train_tf_idf_features)\n",
    "test_tf_idf = pd.DataFrame(test_tf_idf_features)\n",
    "features = ['chars', 'words', 'capital_words', 'sentence_count', 'unique_words', 'stopwords_count']\n",
    "train_Y = train['label']\n",
    "test_Y = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_tf_idf, train[features], left_index=True, right_index=True)\n",
    "test = pd.merge(test_tf_idf, test[features], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, train_Y, test_size=0.2, random_state = 42)# Random Forest Classifier\n",
    "_RandomForestClassifier = RandomForestClassifier(n_estimators = 1000, min_samples_split = 15, random_state = 42)\n",
    "_RandomForestClassifier.fit(X_train, y_train)\n",
    "_RandomForestClassifier_prediction = _RandomForestClassifier.predict(X_test)\n",
    "val_RandomForestClassifier_prediction = _RandomForestClassifier.predict(test)\n",
    "print(\"Accuracy => \", round(accuracy_score(_RandomForestClassifier_prediction, y_test)*100, 2))\n",
    "print(\"\\nRandom Forest Classifier results: \\n\")\n",
    "print(classification_report(y_test, _RandomForestClassifier_prediction, target_names = ['fitzgerald', 'hemingway']))\n",
    "print(\"Validation Accuracy => \", round(accuracy_score(val_RandomForestClassifier_prediction, test_Y)*100, 2))\n",
    "print(\"\\nValidation Random Forest Classifier results: \\n\")\n",
    "print(classification_report(test_Y, val_RandomForestClassifier_prediction, target_names = ['fitzgerald', 'hemingway']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e56c70b3071a7c9e5271d6c05d63446be4cb37f733ae995dda36f1f67e797e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
